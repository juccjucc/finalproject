Machine Learning – final project

1. STEP: Preprocessing:
Examining the data set I noticed that every 20 row there is special row (newwindow=yes) that contains different information respect the other lines. 
It was better to exclude those lines because in the other cases the columns were empty.
I also realized that the first coumns contains information that are irrelevant. For example: X,user_name, timestamp. So I excluded that columns.
After I checked with the nearzerovariance function what are the other columns that does not contanin useful information.
Furthermore I also excluded four columns where the freqRatio were too high.

	row.names	freqRatio	percentUnique	zeroVar	nzv
1	roll_arm	51.15385	13.75937	FALSE	FALSE
2	pitch_arm	85.28205	15.96066	FALSE	FALSE
3	yaw_arm		32.28155	14.89904	FALSE	FALSE
4	pitch_forearm	64.57627	15.09679	FALSE	FALSE


# delete the rows where new_windows is yes because it is a summary and contains not relevant extra info
train=split(train_original, train_original$new_window)$no  

#columns that are not relevant: x, name, timeframe, numwindow
#they are genereal information and not relevant for the experiment
coltodelete <- c("X","user_name","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp","num_window")
ex1=which(names(train) %in% coltodelete)

#columns with near zero variance
ex2=nearZeroVar(train)

#columns with too high freqRatio
nz_details=nearZeroVar(train, saveMetrics = TRUE)  #returns the matrix
nz2 <- subset(nz_details, freqRatio>20)
mycols <- c("roll_arm","pitch_arm","yaw_arm","pitch_forearm")
ex3=which(names(train) %in% mycols)

#all the columns that need to be excluded
exclude=c(ex1,ex2,ex3)

train1=train[,-exclude]


2. STEP: Model fitting:
First I created a sub train and a sub test set, which is a simple hold-out cross validation.

#make sub test and sub training set
inTrain <- createDataPartition(y=train1$classe, p=0.75, list=FALSE) 
subtrain <- train1[inTrain,]
subtest <- train1[-inTrain,]

Apply RANDOM FOREST with ntree=10 to make the algorithm faster.

> #apply random forest
> mod_rf <- train(subtrain$classe ~.,method="rf", data=subtrain, ntree=10)

Random Forest 
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1363    2    2    0    0
         B    6  909   11    3    0
         C    1   12  825    0    0
         D    1    0    7  777    1
         E    0    2    4    4  872

Overall Statistics
                                          
               Accuracy : 0.9883          
                 95% CI : (0.9849, 0.9912)
    No Information Rate : 0.2855          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9852          
 Mcnemar's Test P-Value : NA              



Apply RANDOM FOREST with ntree=10 to make the algorithm faster and cross validation.
I have chosen k-fold cross validation with k=10

> #cross validation k-fold, k=10
> ctrl <- trainControl(method = "cv", number = 10)
> #apply random forest
> mod_rf_cv <- train(subtrain$classe ~.,method="rf", trainControl= ctrl, data=subtrain, ntree=10)

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1367    0    0    0    0
         B    9  913    4    2    1
         C    2   17  815    4    0
         D    1    3   11  769    2
         E    1    2    3    3  873

Overall Statistics
                                          
               Accuracy : 0.9865          
                 95% CI : (0.9828, 0.9895)
    No Information Rate : 0.2874          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9829          
 Mcnemar's Test P-Value : 0.001774        



The accuracy between the hold-out and  k-fold cross validation is very little. So I decided to use only a simple cross validation to calculate the expected out of error rate.

The out of error rate is:  0,0117


 
Apply other prediction methods to compare accuracy: 

Linear Discriminant Analysis 
> conf_lda
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1136   20   91  114    6
         B  150  592  111   35   41
         C  107   73  522  111   25
         D   69   39   86  545   47
         E   42  159   87   84  510

Overall Statistics
                                          
               Accuracy : 0.6883          
                 95% CI : (0.6749, 0.7013)
    No Information Rate : 0.3132          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.6045          
 Mcnemar's Test P-Value : < 2.2e-16       


 
CART Tree based models
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1191    0  171    0    5
         B  615    0  314    0    0
         C  395    0  443    0    0
         D  494    0  292    0    0
         E  182    0  302    0  398

Overall Statistics
                                          
               Accuracy : 0.4232          
                 95% CI : (0.4091, 0.4373)
    No Information Rate : 0.5991          
    P-Value [Acc > NIR] : 1               
                                          
                  Kappa : 0.2397          
 Mcnemar's Test P-Value : NA              

This models does not perform well.
 


3. STEP: Prediction
Final prediction on the test using using random forest.
I think the best prediction model is the random forest because it performs very well respect to other models.
final_pred_rf <- predict(mod_rf, test)
> final_pred_rf
 
[1] B A B A A E D B A A C C B A E E A B B B
Levels: A B C D E

